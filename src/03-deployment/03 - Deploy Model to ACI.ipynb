{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Deploying a model to Azure Container Instance\n",
    "\n",
    "Now that we have trained a set of models and identified the run containing the best model, we want to deploy the model for real time inferencing.  The process of deploying a model involves\n",
    "* registering a model in your workspace\n",
    "* creating a scoring file containing init and run methods\n",
    "* retrieving the environment to run the model\n",
    "* creating \"InferenceConfig\" and \"DeploymentConfig\" objects\n",
    "* _Optionally:_ You can \"Profile\" the model\n",
    "* deploying the model as a Docker image to the deployment target.\n",
    "\n",
    "In this lab, we'll create an Azure Container Instance (ACI) deployed model. This is most suitable for dev/test workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Please enter your username in the `.env` file and run this cell again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a16cb84bf6f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mUSER_NAME\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please enter your username in the `.env` file and run this cell again.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: Please enter your username in the `.env` file and run this cell again."
     ]
    }
   ],
   "source": [
    "import environs\n",
    "\n",
    "e_vars = environs.Env()\n",
    "e_vars.read_env('../workshop.env')\n",
    "\n",
    "USER_NAME = e_vars.str(\"USER_NAME\")\n",
    "EXPERIMENT_NAME = e_vars.str('EXPERIMENT_NAME')\n",
    "ENVIRONMENT_NAME = e_vars.str(\"ENVIRONMENT_NAME\")\n",
    "DATASET_NAME = e_vars.str(\"DATASET_NAME\")\n",
    "\n",
    "SERVICE_NAME = e_vars.str(\"SERVICE_NAME\")\n",
    "MODEL_NAME = e_vars.str(\"MODEL_NAME\")\n",
    "\n",
    "if not USER_NAME:\n",
    "    raise NotImplementedError(\"Please enter your username in the `.env` file and run this cell again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "experiment = Experiment(ws, EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Best Run\n",
    "We can use the SDK to search through our runs to determine which was the best run. In our case, we'll use RMSE to determine the best metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def find_best_run(experiment, metric, goal='minimize'):\n",
    "     runs = {}\n",
    "     run_metrics = {}\n",
    "    \n",
    "     # Create dictionaries containing the runs and the metrics for all runs containing the metric\n",
    "     for r in tqdm(experiment.get_runs(include_children=True)):\n",
    "         metrics = r.get_metrics()\n",
    "         if metric in metrics.keys():\n",
    "             runs[r.id] = r\n",
    "             run_metrics[r.id] = metrics\n",
    "            \n",
    "     if goal == 'minimize':\n",
    "         min_run = min(run_metrics, key=lambda k: run_metrics[k][metric])\n",
    "         return runs[min_run]\n",
    "     else:\n",
    "         max_run = max(run_metrics, key=lambda k: run_metrics[k][metric])\n",
    "         return runs[max_run]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = find_best_run(experiment, 'rmse', 'minimize')\n",
    "\n",
    "# Display the metrics\n",
    "best_run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register a model from best run\n",
    "We have already identified which run contains the \"best model\" by our evaluation criteria.  Each run has a file structure associated with it that contains various files collected during the run.  Since a run can have many outputs we need to tell AML which file from those outputs represents the model that we want to use for our deployment.  We can use the `run.get_file_names()` method to list the files associated with the run, and then use the `run.register_model()` method to place the model in the workspace's model registry.\n",
    "\n",
    "When using `run.register_model()` we supply a `model_name` that is meaningful for our scenario and the `model_path` of the model relative to the run.  In this case, the model path is what is returned from `run.get_file_names()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "query history"
    ]
   },
   "outputs": [],
   "source": [
    "# View the files in the run\n",
    "for f in best_run.get_file_names():\n",
    "    if 'logs' not in f:\n",
    "        print(f)\n",
    "    \n",
    "# Register the model with the workspace\n",
    "model = best_run.register_model(model_name=MODEL_NAME, model_path='outputs/model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a model is registered, it is accessible from the list of models on the AML workspace.  If you register models with the same name multiple times, AML keeps a version history of those models for you.  The `Model.list()` lists all models in a workspace, and can be filtered by name, tags, or model properties.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "register model from history"
    ]
   },
   "outputs": [],
   "source": [
    "# Find all models called \"diabetes_regression_model\" and display their version numbers\n",
    "from azureml.core.model import Model\n",
    "\n",
    "models = Model.list(ws, name=MODEL_NAME)\n",
    "for m in models:\n",
    "    print(m.name, m.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a scoring file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # note here \"best_model\" is the name of the model registered under the workspace\n",
    "    # this call should return the path to the model.pkl file on the local disk.\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.pkl')\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "# note you can pass in multiple rows for scoring\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        data = json.loads(raw_data)['data']\n",
    "        data = np.array(data)\n",
    "        result = model.predict(data)\n",
    "\n",
    "        # you can return any data type as long as it is JSON-serializable\n",
    "        return result.tolist()\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Inference Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "environ = ws.environments[ENVIRONMENT_NAME]\n",
    "\n",
    "inference_cfg = InferenceConfig(entry_script='score.py', environment=environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Deployment Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aci_cfg = AciWebservice.deploy_configuration(cpu_cores=0.5, \n",
    "                                               memory_gb=0.5, \n",
    "                                               tags={'disease': 'diabetes', \n",
    "                                                     'target': 'blood_sugar'}, \n",
    "                                               description='Diabetes Regression Model',\n",
    "                                               auth_enabled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy your webservice\n",
    "**Note:** The web service creation can take several minutes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "# Create the webservice using all of the precreated configurations and our best model\n",
    "service = Model.deploy(workspace=ws, \n",
    "                       name=SERVICE_NAME,\n",
    "                       deployment_config=aci_cfg,\n",
    "                       models=[model],\n",
    "                       inference_config=inference_cfg,\n",
    "                       overwrite=True)\n",
    "\n",
    "# Wait for the service deployment to complete while displaying log output\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test your webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "diabetes_df = pd.read_parquet('../../data/diabetes.parquet')\n",
    "y = diabetes_df.pop('target').values\n",
    "X = diabetes_df.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Webser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# scrape the first row from the test set.\n",
    "test_samples = json.dumps({\"data\": X_test[0:1, :].tolist()})\n",
    "\n",
    "#score on our service\n",
    "service.run(input_data = test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell shows how you can send multiple rows to the webservice at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "# score 5 rows from the test set.\n",
    "test_samples = json.dumps({'data': X_test.tolist()[:5]})\n",
    "\n",
    "service.run(input_data = test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell shows how you can use the `service.scoring_uri` property to access the HTTP endpoint of the service and call it using standard POST operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# use the first row from the test set again\n",
    "test_samples = json.dumps({\"data\": X_test[0:1, :].tolist()})\n",
    "\n",
    "# create the required header\n",
    "headers = {'Content-Type':'application/json', \"Authorization\": f\"Bearer {service.get_keys()[0]}\"}\n",
    "\n",
    "print(f\"POST request:\")\n",
    "print(f\"    URL: {service.scoring_uri}\")\n",
    "print(f\"    Headers:\")\n",
    "print(f\"         Authorization: {headers['Authorization']}\")\n",
    "print(f\"          Content-Type: {headers['Content-Type']}\")\n",
    "print(\"     Content:\")\n",
    "print(f\"          {test_samples}\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "# post the request to the service and display the result\n",
    "resp = requests.post(service.scoring_uri, test_samples, headers = headers)\n",
    "\n",
    "print(f\"Response from Webservice: {resp.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the ACI instance to stop the compute and any associated billing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "###### Licensed under the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
